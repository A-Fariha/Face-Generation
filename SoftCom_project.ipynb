{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbT-6LYg3UEX",
        "outputId": "31b02109-85c7-4b39-dd5b-65c9b9b6aea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "metadata": {
        "id": "vD6s8PR0v96y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import ReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "vbM1WcYx3UK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "GENERATE_RES = 3 \n",
        "\n",
        "GENERATE_SQUARE = 32 * GENERATE_RES \n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "# Preview image \n",
        "PREVIEW_ROWS = 4\n",
        "PREVIEW_COLS = 7\n",
        "PREVIEW_MARGIN = 16\n",
        "\n",
        "\n",
        "SEED_SIZE = 100\n",
        "\n",
        "\n",
        "DATA_PATH = '/content/drive/My Drive/projects/images'\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 60000\n",
        "\n",
        "print(f\"Will generate {GENERATE_SQUARE}px square images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoAInkql3nCU",
        "outputId": "56677157-6c89-4eac-e2fd-11c85951605c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will generate 96px square images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_binary_path = os.path.join(DATA_PATH,\n",
        "        f'training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n",
        "\n",
        "print(f\"Looking for file: {training_binary_path}\")\n",
        "\n",
        "if not os.path.isfile(training_binary_path):\n",
        "  start = time.time()\n",
        "  print(\"Loading training images...\")\n",
        "\n",
        "  training_data = []\n",
        "  faces_path = os.path.join(DATA_PATH)\n",
        "  for filename in tqdm(os.listdir(faces_path)):\n",
        "      path = os.path.join(faces_path,filename)\n",
        "      image = Image.open(path).resize((GENERATE_SQUARE,\n",
        "            GENERATE_SQUARE),Image.ANTIALIAS)\n",
        "      training_data.append(np.asarray(image))\n",
        "  training_data = np.reshape(training_data,(-1,GENERATE_SQUARE,\n",
        "            GENERATE_SQUARE,IMAGE_CHANNELS))\n",
        "  training_data = training_data.astype(np.float32)\n",
        "  training_data = training_data / 127.5 - 1.\n",
        "\n",
        "\n",
        "  print(\"Saving training image binary...\")\n",
        "  np.save(training_binary_path,training_data)\n",
        "  elapsed = time.time()-start\n",
        "  print (f'Image preprocess time: {hms_string(elapsed)}')\n",
        "else:\n",
        "  print(\"Loading previous training pickle...\")\n",
        "  training_data = np.load(training_binary_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjPNNwnC4mM-",
        "outputId": "247e033b-3a4a-42ed-f4ee-096d0122b8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for file: /content/drive/My Drive/projects/images/training_data_96_96.npy\n",
            "Loading previous training pickle...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(training_data) \\\n",
        "    .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "VV1Qc7i1Ibs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(seed_size, channels):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n",
        "    model.add(Reshape((4,4,256)))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "   \n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "  \n",
        "    \n",
        "    \n",
        "\n",
        "    if GENERATE_RES>1:\n",
        "      model.add(UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n",
        "      model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "      model.add(BatchNormalization(momentum=0.8))\n",
        "      model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "c1S-ZSbxIkFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(image_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \n",
        "                     padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(1024, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(1, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wy6e_D3FIlnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_images(cnt,noise):\n",
        "  image_array = np.full(( \n",
        "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
        "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), IMAGE_CHANNELS), \n",
        "      255, dtype=np.uint8)\n",
        "  \n",
        "  generated_images = generator.predict(noise)\n",
        "\n",
        "  generated_images = 0.5 * generated_images + 0.5\n",
        "\n",
        "  image_count = 0\n",
        "  for row in range(PREVIEW_ROWS):\n",
        "      for col in range(PREVIEW_COLS):\n",
        "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n",
        "            = generated_images[image_count] * 255\n",
        "        image_count += 1\n",
        "\n",
        "          \n",
        "  output_path = os.path.join(DATA_PATH,'output3')\n",
        "  if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "  \n",
        "  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
        "  im = Image.fromarray(image_array)\n",
        "  im.save(filename)"
      ],
      "metadata": {
        "id": "is16f2J1JDT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\n",
        "\n",
        "noise = tf.random.normal([1, SEED_SIZE])\n",
        "generated_image = generator(noise, training=False)\n"
      ],
      "metadata": {
        "id": "ZALsQbsbJHLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n",
        "\n",
        "discriminator = build_discriminator(image_shape)\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lARaJf6ZJTYk",
        "outputId": "8e4294b2-10d2-47e6-99e8-c313b098de19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "metadata": {
        "id": "XcZUxeaUJTeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n"
      ],
      "metadata": {
        "id": "W0wNpCRsJZJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_images = generator(seed, training=True)\n",
        "\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(\\\n",
        "        gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(\\\n",
        "        disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(\n",
        "        gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(\n",
        "        gradients_of_discriminator, \n",
        "        discriminator.trainable_variables))\n",
        "  return gen_loss,disc_loss"
      ],
      "metadata": {
        "id": "V4HM_X0zJdgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B_KShmotJeuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, \n",
        "                                       SEED_SIZE))\n",
        "  start = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    gen_loss_list = []\n",
        "    disc_loss_list = []\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      t = train_step(image_batch)\n",
        "      gen_loss_list.append(t[0])\n",
        "      disc_loss_list.append(t[1])\n",
        "\n",
        "    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
        "    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
        "\n",
        "    epoch_elapsed = time.time()-epoch_start\n",
        "    print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss},'\\\n",
        "           f' {hms_string(epoch_elapsed)}')\n",
        "    save_images(epoch,fixed_seed)\n",
        "\n",
        "    elapsed = time.time()-start\n",
        "\n",
        "\n",
        "    epoch_elapsed = time.time()-epoch_start\n",
        "    save_images(epoch,fixed_seed)\n",
        "\n",
        "  elapsed = time.time()-start\n"
      ],
      "metadata": {
        "id": "_Mrwa0-BJe9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "metadata": {
        "id": "dL7LxsN3JhpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1bed51-9fda-4626-802e-0e55106547e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, gen loss=4.098332405090332,disc loss=1.2123312950134277, 0:00:49.50\n",
            "Epoch 2, gen loss=4.798331260681152,disc loss=1.0113592147827148, 0:00:41.85\n",
            "Epoch 3, gen loss=4.795515060424805,disc loss=1.0502068996429443, 0:00:41.89\n",
            "Epoch 4, gen loss=3.5179619789123535,disc loss=1.1795294284820557, 0:00:41.79\n",
            "Epoch 5, gen loss=3.7103776931762695,disc loss=1.1814924478530884, 0:00:41.96\n",
            "Epoch 6, gen loss=3.712599515914917,disc loss=1.1251198053359985, 0:00:41.78\n",
            "Epoch 7, gen loss=3.8278651237487793,disc loss=0.9614123702049255, 0:00:41.81\n",
            "Epoch 8, gen loss=4.02007532119751,disc loss=0.9357507228851318, 0:00:42.01\n",
            "Epoch 9, gen loss=4.413749694824219,disc loss=1.009212851524353, 0:00:42.01\n",
            "Epoch 10, gen loss=4.569704055786133,disc loss=0.8451287746429443, 0:00:41.84\n",
            "Epoch 11, gen loss=4.6169915199279785,disc loss=0.8748200535774231, 0:01:22.39\n",
            "Epoch 12, gen loss=4.684412956237793,disc loss=0.7929462194442749, 0:00:42.52\n",
            "Epoch 13, gen loss=4.542117118835449,disc loss=0.6906010508537292, 0:00:41.44\n",
            "Epoch 14, gen loss=4.848080158233643,disc loss=0.7453963160514832, 0:00:41.73\n",
            "Epoch 15, gen loss=5.080834865570068,disc loss=0.6824676990509033, 0:00:41.76\n",
            "Epoch 16, gen loss=5.20697546005249,disc loss=0.6865800619125366, 0:00:41.56\n",
            "Epoch 17, gen loss=5.335635185241699,disc loss=0.7264350056648254, 0:00:41.67\n",
            "Epoch 18, gen loss=5.234443664550781,disc loss=0.712695300579071, 0:00:41.79\n",
            "Epoch 19, gen loss=5.251433849334717,disc loss=0.6725899577140808, 0:00:41.77\n",
            "Epoch 20, gen loss=5.385390281677246,disc loss=0.5725127458572388, 0:00:41.69\n",
            "Epoch 21, gen loss=5.6682610511779785,disc loss=0.6102323532104492, 0:00:41.71\n",
            "Epoch 22, gen loss=5.631342887878418,disc loss=0.6235604882240295, 0:00:41.80\n",
            "Epoch 23, gen loss=5.845889091491699,disc loss=0.6309689879417419, 0:00:41.81\n",
            "Epoch 24, gen loss=5.780604839324951,disc loss=0.538618803024292, 0:00:41.71\n",
            "Epoch 25, gen loss=6.054459095001221,disc loss=0.5642760396003723, 0:00:41.66\n",
            "Epoch 26, gen loss=5.91774320602417,disc loss=0.5669898390769958, 0:00:41.71\n",
            "Epoch 27, gen loss=6.059432029724121,disc loss=0.6995351314544678, 0:00:41.76\n",
            "Epoch 28, gen loss=5.948048114776611,disc loss=0.5176671147346497, 0:00:41.75\n",
            "Epoch 29, gen loss=6.054673671722412,disc loss=0.7302587032318115, 0:00:41.72\n",
            "Epoch 30, gen loss=5.909732818603516,disc loss=0.44372546672821045, 0:00:41.77\n",
            "Epoch 31, gen loss=6.281335830688477,disc loss=0.6910665035247803, 0:00:41.79\n",
            "Epoch 32, gen loss=6.355202674865723,disc loss=0.49811312556266785, 0:01:22.37\n",
            "Epoch 33, gen loss=6.086477279663086,disc loss=0.7322779893875122, 0:00:42.39\n",
            "Epoch 34, gen loss=6.273592472076416,disc loss=0.45662155747413635, 0:00:41.43\n",
            "Epoch 35, gen loss=6.5242156982421875,disc loss=0.44496870040893555, 0:00:41.69\n",
            "Epoch 36, gen loss=6.652713298797607,disc loss=0.6160002946853638, 0:00:41.73\n",
            "Epoch 37, gen loss=6.601560115814209,disc loss=0.4018678367137909, 0:00:41.53\n",
            "Epoch 38, gen loss=6.768751621246338,disc loss=0.44956716895103455, 0:00:41.71\n",
            "Epoch 39, gen loss=6.936513900756836,disc loss=0.3623378872871399, 0:00:41.70\n",
            "Epoch 40, gen loss=7.156419277191162,disc loss=0.4236263334751129, 0:00:41.65\n",
            "Epoch 41, gen loss=6.960464954376221,disc loss=0.5509700179100037, 0:00:41.71\n",
            "Epoch 42, gen loss=6.897734642028809,disc loss=0.4566844701766968, 0:00:41.71\n",
            "Epoch 43, gen loss=7.358216762542725,disc loss=0.4060671329498291, 0:00:41.77\n",
            "Epoch 44, gen loss=7.53389835357666,disc loss=0.4881848394870758, 0:00:41.60\n",
            "Epoch 45, gen loss=7.1816816329956055,disc loss=0.645806074142456, 0:00:41.57\n",
            "Epoch 46, gen loss=7.431385040283203,disc loss=0.4371573030948639, 0:00:41.63\n",
            "Epoch 47, gen loss=7.800175189971924,disc loss=0.4565151333808899, 0:00:41.71\n",
            "Epoch 48, gen loss=7.62164831161499,disc loss=0.22244353592395782, 0:00:41.78\n",
            "Epoch 49, gen loss=7.641897201538086,disc loss=0.41968151926994324, 0:00:41.84\n",
            "Epoch 50, gen loss=7.643489837646484,disc loss=0.6021856069564819, 0:00:41.71\n",
            "Epoch 51, gen loss=7.741450786590576,disc loss=0.5465724468231201, 0:00:41.67\n",
            "Epoch 52, gen loss=7.680541515350342,disc loss=0.38004210591316223, 0:00:41.69\n",
            "Epoch 53, gen loss=7.951071739196777,disc loss=0.3194560110569, 0:00:41.75\n",
            "Epoch 54, gen loss=8.167686462402344,disc loss=0.3058494031429291, 0:00:41.76\n",
            "Epoch 55, gen loss=8.25472354888916,disc loss=0.3251538574695587, 0:00:41.82\n",
            "Epoch 56, gen loss=8.441908836364746,disc loss=0.4649193584918976, 0:00:41.81\n",
            "Epoch 57, gen loss=8.387301445007324,disc loss=0.26648443937301636, 0:00:41.79\n",
            "Epoch 58, gen loss=8.755946159362793,disc loss=0.40975746512413025, 0:00:41.80\n",
            "Epoch 59, gen loss=8.706491470336914,disc loss=0.23645555973052979, 0:00:41.76\n",
            "Epoch 60, gen loss=8.949352264404297,disc loss=0.2621074914932251, 0:00:41.72\n",
            "Epoch 61, gen loss=8.844152450561523,disc loss=0.28620144724845886, 0:00:41.71\n",
            "Epoch 62, gen loss=9.019639015197754,disc loss=0.46470627188682556, 0:00:41.68\n",
            "Epoch 63, gen loss=8.750131607055664,disc loss=0.5982295274734497, 0:00:41.70\n",
            "Epoch 64, gen loss=8.886631965637207,disc loss=0.5291905999183655, 0:00:41.83\n",
            "Epoch 65, gen loss=8.679719924926758,disc loss=0.5294319987297058, 0:00:41.70\n",
            "Epoch 66, gen loss=8.853137016296387,disc loss=0.30885767936706543, 0:00:41.66\n",
            "Epoch 67, gen loss=8.841523170471191,disc loss=0.23723913729190826, 0:00:42.05\n",
            "Epoch 68, gen loss=8.726046562194824,disc loss=0.5882435441017151, 0:00:41.72\n",
            "Epoch 69, gen loss=8.90964412689209,disc loss=0.3503037691116333, 0:00:41.75\n",
            "Epoch 70, gen loss=9.27718448638916,disc loss=0.17063885927200317, 0:00:41.80\n",
            "Epoch 71, gen loss=9.339475631713867,disc loss=0.22140486538410187, 0:00:41.76\n",
            "Epoch 72, gen loss=9.336227416992188,disc loss=0.502111554145813, 0:00:41.77\n",
            "Epoch 73, gen loss=9.27164077758789,disc loss=0.42865461111068726, 0:00:41.82\n",
            "Epoch 74, gen loss=9.349960327148438,disc loss=0.18138210475444794, 0:00:41.75\n",
            "Epoch 75, gen loss=9.222623825073242,disc loss=0.4893760085105896, 0:00:41.74\n",
            "Epoch 76, gen loss=9.58326244354248,disc loss=0.41697603464126587, 0:00:41.76\n",
            "Epoch 77, gen loss=9.234496116638184,disc loss=0.2590498626232147, 0:00:41.75\n",
            "Epoch 78, gen loss=9.535684585571289,disc loss=0.2704220116138458, 0:00:41.70\n",
            "Epoch 79, gen loss=9.107743263244629,disc loss=0.6528827548027039, 0:00:41.70\n",
            "Epoch 80, gen loss=9.531086921691895,disc loss=0.15816190838813782, 0:00:41.76\n",
            "Epoch 81, gen loss=9.136648178100586,disc loss=0.562025249004364, 0:00:41.84\n",
            "Epoch 82, gen loss=9.820171356201172,disc loss=0.20902876555919647, 0:00:41.79\n",
            "Epoch 83, gen loss=9.602568626403809,disc loss=0.7124086618423462, 0:00:41.71\n",
            "Epoch 84, gen loss=9.67564868927002,disc loss=0.24993464350700378, 0:00:41.66\n",
            "Epoch 85, gen loss=9.429258346557617,disc loss=0.4853219985961914, 0:00:41.70\n",
            "Epoch 86, gen loss=9.656158447265625,disc loss=0.17354734241962433, 0:00:41.77\n",
            "Epoch 87, gen loss=9.889602661132812,disc loss=0.38590022921562195, 0:00:41.76\n",
            "Epoch 88, gen loss=9.816067695617676,disc loss=0.17018820345401764, 0:00:41.75\n",
            "Epoch 89, gen loss=10.285205841064453,disc loss=0.2591020464897156, 0:00:41.74\n",
            "Epoch 90, gen loss=10.17310619354248,disc loss=0.2686631977558136, 0:00:41.76\n",
            "Epoch 91, gen loss=10.295011520385742,disc loss=0.34653034806251526, 0:00:41.75\n",
            "Epoch 92, gen loss=10.030121803283691,disc loss=0.2336091548204422, 0:00:41.74\n",
            "Epoch 93, gen loss=10.333975791931152,disc loss=0.2973334491252899, 0:00:41.78\n",
            "Epoch 94, gen loss=10.496339797973633,disc loss=0.2755955755710602, 0:00:41.72\n",
            "Epoch 95, gen loss=10.253316879272461,disc loss=0.4604252874851227, 0:00:41.68\n",
            "Epoch 96, gen loss=10.454523086547852,disc loss=0.25161394476890564, 0:00:41.68\n",
            "Epoch 97, gen loss=10.496542930603027,disc loss=0.17106257379055023, 0:00:41.71\n",
            "Epoch 98, gen loss=10.454615592956543,disc loss=0.4936603605747223, 0:00:41.71\n",
            "Epoch 99, gen loss=10.45506763458252,disc loss=0.23266124725341797, 0:00:41.65\n",
            "Epoch 100, gen loss=10.687995910644531,disc loss=0.26456665992736816, 0:00:41.67\n",
            "Epoch 101, gen loss=10.451120376586914,disc loss=0.4424276649951935, 0:00:41.94\n",
            "Epoch 102, gen loss=10.15921401977539,disc loss=0.38334959745407104, 0:00:41.74\n",
            "Epoch 103, gen loss=10.639161109924316,disc loss=0.1831664890050888, 0:00:41.78\n",
            "Epoch 104, gen loss=10.762153625488281,disc loss=0.25285011529922485, 0:00:41.72\n",
            "Epoch 105, gen loss=10.772167205810547,disc loss=0.25492987036705017, 0:00:41.72\n",
            "Epoch 106, gen loss=10.793282508850098,disc loss=0.14445747435092926, 0:00:41.74\n",
            "Epoch 107, gen loss=11.259811401367188,disc loss=0.17999757826328278, 0:00:41.72\n",
            "Epoch 108, gen loss=10.964985847473145,disc loss=0.40556809306144714, 0:00:41.73\n",
            "Epoch 109, gen loss=11.064324378967285,disc loss=0.16171668469905853, 0:00:41.73\n",
            "Epoch 110, gen loss=10.58484172821045,disc loss=0.6370853185653687, 0:00:41.74\n",
            "Epoch 111, gen loss=10.547212600708008,disc loss=0.45675840973854065, 0:00:41.79\n",
            "Epoch 112, gen loss=10.757092475891113,disc loss=0.18559426069259644, 0:00:41.76\n",
            "Epoch 113, gen loss=10.674795150756836,disc loss=0.4431540071964264, 0:00:41.68\n",
            "Epoch 114, gen loss=10.67094612121582,disc loss=0.15624436736106873, 0:00:41.73\n",
            "Epoch 115, gen loss=10.891157150268555,disc loss=0.264174222946167, 0:00:41.81\n",
            "Epoch 116, gen loss=11.238856315612793,disc loss=0.22577309608459473, 0:00:41.75\n",
            "Epoch 117, gen loss=10.386197090148926,disc loss=0.4712315499782562, 0:00:41.69\n",
            "Epoch 118, gen loss=10.92497730255127,disc loss=0.17556798458099365, 0:00:41.66\n",
            "Epoch 119, gen loss=11.387269973754883,disc loss=0.138213649392128, 0:00:41.63\n",
            "Epoch 120, gen loss=11.723922729492188,disc loss=0.11682049185037613, 0:00:41.52\n",
            "Epoch 121, gen loss=11.814202308654785,disc loss=0.25573500990867615, 0:00:41.44\n",
            "Epoch 122, gen loss=11.480205535888672,disc loss=0.5694693326950073, 0:00:41.42\n",
            "Epoch 123, gen loss=11.704461097717285,disc loss=0.23910699784755707, 0:00:41.51\n",
            "Epoch 124, gen loss=11.722533226013184,disc loss=0.17057853937149048, 0:00:41.57\n",
            "Epoch 125, gen loss=11.704291343688965,disc loss=0.3492587208747864, 0:00:41.58\n",
            "Epoch 126, gen loss=11.572612762451172,disc loss=0.2766967713832855, 0:00:41.57\n",
            "Epoch 127, gen loss=11.465883255004883,disc loss=0.11596076935529709, 0:00:41.57\n",
            "Epoch 128, gen loss=11.88308334350586,disc loss=0.09332526475191116, 0:00:41.55\n",
            "Epoch 129, gen loss=12.253767967224121,disc loss=0.2306189388036728, 0:00:41.55\n",
            "Epoch 130, gen loss=12.436650276184082,disc loss=0.1559031456708908, 0:00:41.53\n",
            "Epoch 131, gen loss=12.761979103088379,disc loss=0.1523224115371704, 0:00:41.54\n",
            "Epoch 132, gen loss=12.647963523864746,disc loss=0.12935771048069, 0:00:41.53\n",
            "Epoch 133, gen loss=13.092633247375488,disc loss=0.18351128697395325, 0:00:41.59\n",
            "Epoch 134, gen loss=12.81919002532959,disc loss=0.29842817783355713, 0:00:41.49\n",
            "Epoch 135, gen loss=12.690926551818848,disc loss=0.2706495523452759, 0:00:41.41\n",
            "Epoch 136, gen loss=12.49878215789795,disc loss=0.10659916698932648, 0:00:41.45\n",
            "Epoch 137, gen loss=12.774462699890137,disc loss=0.22765609622001648, 0:00:41.54\n",
            "Epoch 138, gen loss=11.601369857788086,disc loss=0.42847850918769836, 0:00:41.64\n",
            "Epoch 139, gen loss=12.766441345214844,disc loss=0.21255558729171753, 0:00:41.71\n",
            "Epoch 140, gen loss=12.632390975952148,disc loss=0.48083382844924927, 0:00:41.74\n",
            "Epoch 141, gen loss=12.747398376464844,disc loss=0.21291835606098175, 0:00:41.68\n",
            "Epoch 142, gen loss=12.591391563415527,disc loss=0.2188868522644043, 0:00:41.64\n",
            "Epoch 143, gen loss=12.727453231811523,disc loss=0.24238167703151703, 0:00:41.72\n",
            "Epoch 144, gen loss=12.814543724060059,disc loss=0.17892642319202423, 0:00:41.62\n",
            "Epoch 145, gen loss=12.746017456054688,disc loss=0.12791086733341217, 0:00:41.68\n",
            "Epoch 146, gen loss=12.886687278747559,disc loss=0.24971316754817963, 0:00:41.64\n",
            "Epoch 147, gen loss=13.084611892700195,disc loss=0.10996000468730927, 0:00:41.71\n",
            "Epoch 148, gen loss=13.56205940246582,disc loss=0.19037635624408722, 0:00:41.72\n",
            "Epoch 149, gen loss=13.118925094604492,disc loss=0.14721038937568665, 0:00:41.75\n",
            "Epoch 150, gen loss=12.650545120239258,disc loss=0.4092685580253601, 0:01:22.38\n",
            "Epoch 151, gen loss=13.125509262084961,disc loss=0.13459299504756927, 0:00:42.40\n",
            "Epoch 152, gen loss=13.63428020477295,disc loss=0.1313340812921524, 0:00:41.38\n",
            "Epoch 153, gen loss=13.153727531433105,disc loss=0.07510101795196533, 0:00:41.63\n",
            "Epoch 154, gen loss=14.022478103637695,disc loss=0.1903652548789978, 0:00:41.70\n",
            "Epoch 155, gen loss=13.93673324584961,disc loss=0.17276924848556519, 0:00:41.47\n",
            "Epoch 156, gen loss=13.140573501586914,disc loss=0.5637308359146118, 0:00:41.54\n",
            "Epoch 157, gen loss=12.417730331420898,disc loss=0.4804030954837799, 0:00:41.74\n",
            "Epoch 158, gen loss=11.84048843383789,disc loss=0.42455655336380005, 0:00:41.57\n",
            "Epoch 159, gen loss=12.840302467346191,disc loss=0.1764325201511383, 0:00:41.58\n",
            "Epoch 160, gen loss=12.783621788024902,disc loss=0.15152212977409363, 0:00:41.62\n",
            "Epoch 161, gen loss=12.832850456237793,disc loss=0.19372990727424622, 0:00:41.70\n",
            "Epoch 162, gen loss=13.510848045349121,disc loss=0.14652740955352783, 0:00:41.56\n",
            "Epoch 163, gen loss=13.299554824829102,disc loss=0.14313945174217224, 0:00:41.53\n",
            "Epoch 164, gen loss=13.300899505615234,disc loss=0.09390904754400253, 0:00:41.62\n",
            "Epoch 165, gen loss=14.074655532836914,disc loss=0.24987593293190002, 0:00:41.62\n",
            "Epoch 166, gen loss=13.8700590133667,disc loss=0.10067591071128845, 0:01:22.37\n",
            "Epoch 167, gen loss=14.830484390258789,disc loss=0.13055197894573212, 0:00:42.30\n",
            "Epoch 168, gen loss=14.379100799560547,disc loss=0.11105424165725708, 0:00:41.32\n",
            "Epoch 169, gen loss=13.729426383972168,disc loss=0.11216755956411362, 0:00:41.53\n",
            "Epoch 170, gen loss=13.584724426269531,disc loss=0.6871970295906067, 0:00:41.63\n",
            "Epoch 171, gen loss=12.842796325683594,disc loss=0.16881246864795685, 0:00:41.39\n",
            "Epoch 172, gen loss=13.000429153442383,disc loss=0.4510510563850403, 0:00:41.50\n",
            "Epoch 173, gen loss=13.086933135986328,disc loss=0.10001537948846817, 0:00:41.67\n",
            "Epoch 174, gen loss=13.912580490112305,disc loss=0.11035068333148956, 0:00:41.47\n",
            "Epoch 175, gen loss=13.918286323547363,disc loss=0.22321200370788574, 0:00:41.50\n",
            "Epoch 176, gen loss=14.626012802124023,disc loss=0.21629425883293152, 0:00:41.68\n",
            "Epoch 177, gen loss=14.263460159301758,disc loss=0.2285500168800354, 0:00:41.47\n",
            "Epoch 178, gen loss=14.064647674560547,disc loss=0.1452256143093109, 0:00:41.40\n",
            "Epoch 179, gen loss=14.576252937316895,disc loss=0.30557721853256226, 0:00:41.43\n",
            "Epoch 180, gen loss=14.204894065856934,disc loss=0.12582513689994812, 0:00:41.56\n",
            "Epoch 181, gen loss=14.531912803649902,disc loss=0.23423896729946136, 0:00:41.61\n",
            "Epoch 182, gen loss=13.816398620605469,disc loss=0.15097735822200775, 0:00:41.64\n",
            "Epoch 183, gen loss=13.85083293914795,disc loss=0.17078453302383423, 0:00:41.55\n",
            "Epoch 184, gen loss=14.185857772827148,disc loss=0.12939146161079407, 0:00:41.49\n",
            "Epoch 185, gen loss=15.34355354309082,disc loss=0.21116475760936737, 0:00:41.51\n",
            "Epoch 186, gen loss=14.35610294342041,disc loss=0.33267560601234436, 0:00:41.63\n",
            "Epoch 187, gen loss=14.455580711364746,disc loss=0.11072670668363571, 0:00:41.63\n",
            "Epoch 188, gen loss=14.548017501831055,disc loss=0.14781710505485535, 0:00:41.57\n",
            "Epoch 189, gen loss=14.920326232910156,disc loss=0.17826178669929504, 0:00:41.49\n",
            "Epoch 190, gen loss=14.921643257141113,disc loss=0.11202895641326904, 0:00:41.50\n",
            "Epoch 191, gen loss=14.453534126281738,disc loss=0.32513779401779175, 0:00:41.62\n",
            "Epoch 192, gen loss=14.47163200378418,disc loss=0.11765926331281662, 0:00:41.59\n",
            "Epoch 193, gen loss=15.758694648742676,disc loss=0.17121347784996033, 0:00:41.60\n",
            "Epoch 194, gen loss=14.506983757019043,disc loss=0.26070407032966614, 0:00:41.59\n",
            "Epoch 195, gen loss=14.550864219665527,disc loss=0.11277447640895844, 0:00:41.61\n",
            "Epoch 196, gen loss=15.010953903198242,disc loss=0.12021707743406296, 0:00:41.64\n",
            "Epoch 197, gen loss=15.395133018493652,disc loss=0.26447904109954834, 0:00:41.64\n",
            "Epoch 198, gen loss=15.438202857971191,disc loss=0.32367709279060364, 0:00:41.62\n",
            "Epoch 199, gen loss=14.604841232299805,disc loss=0.10969183593988419, 0:00:41.55\n",
            "Epoch 200, gen loss=15.709938049316406,disc loss=0.2532118558883667, 0:00:41.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(g_loss,label=\"G\")\n",
        "plt.plot(d_loss,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "SoPSHOyT4ffG",
        "outputId": "bcccec70-c1e3-4c9f-9a7b-1b5051bbfe0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-178f20ab4139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generator and Discriminator Loss During Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iterations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'g_loss' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}